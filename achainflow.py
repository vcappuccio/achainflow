#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
achainflow.py

This script provides a Streamlit application that consults a chain of advisors using different AI models to solve coding problems and technical questions.
 
"""

import os
import asyncio
from dotenv import load_dotenv

import streamlit as st
from groq import Groq
import ollama
from ollama import AsyncClient

load_dotenv()

groq_api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key=groq_api_key)


def go_groq_go(messages, system_message):
    """
    Send a message to the Groq API for completion and return the response message content.
    
    Args:
        messages (str): The user message to be completed.
        system_message3 (str): The system message to be included in the completion.

    Returns:
        str: The completed message content.
    """
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": messages},
        ],
        model="llama3-70b-8192",
        temperature=0.1,
        max_tokens=1024,
    )
    message_content = response.choices[0].message.content.strip()
    return message_content


async def consult_advisor_async(model, system_message, user_message):
    """
    Asynchronously consult an advisor using the Ollama API and return the response message content.
    
    Args:
        model (str): The model to consult.
        system_message (str): The system message to be included in the consultation.
        user_message (str or list): The user message(s) to be included in the consultation.

    Returns:
        str: The advisor's response message content.
    """
    messages = [{"role": "system", "content": system_message}]
    
    if isinstance(user_message, list):
        messages.extend(
            {"role": "user", "content": msg} if isinstance(msg, str) else msg
            for msg in user_message
        )
    else:
        messages.append({"role": "user", "content": user_message})
    
    response = await AsyncClient().chat(model=model, messages=messages)
    return response['message']['content'].strip()


async def get_over_a_chain_of_advisors_async(user_message):
    """
    Consult a chain of advisors using different models and return the final answer.
    
    Args:
        user_message (str): The user message to be consulted by the chain of advisors.

    Returns:
        str: The final answer generated by the chain of advisors.
    """
    system_message = """
    You are an Arquitech, an advanced AI assistant and code problem solver. Your only purpose is to help find the best solutions to any coding challenges and technical questions.
    Let's get started! What challenge can I help you with today?
    """
    models = {
        "phi3": "Microsoft PHI3",
        "mistral": "Mistral 7B",
        "gemma:latest": "Gemma 7B",
        "dolphin-mixtral:latest": "Dolphin Mixtral",
        "dolphin2.2-mistral:latest": "Dolphin2.2 Mistral",
        "codebooga:latest": "Codebooga",
    }
    
    advisors_answers = {}

    for model, name in models.items():
        st.markdown("---")
        st.markdown(f"### Processing {name}'s advice:")
        st.markdown("---")
        advice = await consult_advisor_async(model, system_message, user_message)
        st.markdown(advice)

        advisors_answers[name] = f"Provided the following advice:\n\n'''{advice}'''\n\n"

    formatted_advisors_answers = ""
    for i, (advisor, answer) in enumerate(advisors_answers.items()):
        formatted_advisors_answers += f"|{i+1}| Start of Advisor: {advisor}: \n"
        formatted_advisors_answers += f"his answer:\n"
        formatted_advisors_answers += f"'''{answer}'''\n"
        formatted_advisors_answers += f"End of Advisor: {advisor} |"

    advisor_evaluation = f"""
    \n\n
    I need the most comprehensive and insightful answers, from the advisors who offer their unique perspectives and knowledge. 
    These advisors are specialized AI models, each with their own areas of expertise.
    
    You will carefully consider each advisor's input, analyzing their suggestions and weighing their relevance to the specific problem.
    
    You will synthesize their ideas with your own understanding and problem-solving capabilities. 
    
    You will aim to provide with a well-reasoned, step-by-step plan to tackle your challenge, drawing upon the collective wisdom of my advisors and my own expertise.
    
    Here are the advisors names and answers: \n
    {formatted_advisors_answers}

    for this problem:\n
    {user_message}
    \n
    
    Use the insights from the advisors to create a step-by-step plan to solve the given problem,
    then solve the problem your way taking all the advisors insights into account. Also, include footnotes to the best advisor contributions."""


    final_answer =  go_groq_go(system_message, advisor_evaluation)

    st.markdown("---")
    return final_answer


async def main():
    """
    Main function to run the Streamlit application.
    """
    st.title("Multitude of Advisors")

    problem = st.text_area("Paste your problem: ")

    if st.button("Solve my problems"):
        with st.spinner("Broadcasting this problem to advisors..."):
            final_answer = await get_over_a_chain_of_advisors_async(problem)

        st.header("Distributed Answer")
        st.markdown(final_answer)


if __name__ == "__main__":
    asyncio.run(main())
